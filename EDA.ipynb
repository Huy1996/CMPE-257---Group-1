{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Filter Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_to_dict(file_path):\n",
    "    \"\"\"Reads a YAML file and returns its content as a dictionary.\"\"\"\n",
    "    import yaml\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return None\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_yaml_to_dict(\"Modified Data/variable.yaml\")\n",
    "df = pd.read_csv(\"Modified Data/imputed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "  # Binary\n",
    "  \"SEXVAR\": \"Gender\",\n",
    "  \"BPHIGH6\": \"High Blood Pressure\",\n",
    "  \"CHCKDNY2\": \"Kidney Disease\",\n",
    "  \"CHOLMED3\": \"Taking medicine for high cholesterol\",\n",
    "  \"CVDCRHD4\": \"Angina or Coronary Heart Disease\",\n",
    "  \"CVDSTRK3\": \"Stroke\",\n",
    "  \"DIABETE4\": \"Diabetes\",\n",
    "  \"HAVARTH4\": \"Arthritis\",\n",
    "  \"SMOKE100\": \"Smoked at Least 100 Cigarettes\",\n",
    "  \"TOLDHI3\": \"Cholesterol Is High\",\n",
    "  \"EXERANY2\": \"Exercise in Past 30 Days\",\n",
    "  \"CVDINFR4\": \"Heart Attack\",  # Target\n",
    "  # Ordinal\n",
    "  \"ECIGNOW2\": \"E-cigarettes Frequency\",\n",
    "  \"GENHLTH\": \"General Health\",\n",
    "  \"USENOW3\": \"Smokeless Tobacco Products\",\n",
    "  \"_AGEG5YR\": \"Age Range\",\n",
    "  # Numeric\n",
    "  \"_AGE80\": \"Age\",\n",
    "  \"PHYSHLTH\": \"Number of Days Physical Health Not Good\",\n",
    "  \"MENTHLTH\": \"Number of Days Mental Health Not Good\",\n",
    "  \"STRENGTH\": \"Physical activities frequence\",\n",
    "  \"ALCDAY4\": \"Days in past 30 had alcoholic beverage\",\n",
    "  \"WEIGHT2\": \"Weight in Pounds\",\n",
    "  \"HEIGHT3\": \"Reported Height in Feet\",\n",
    "  \"_BMI5\": \"BMI\"\n",
    "}\n",
    "df = df.rename(columns=rename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"].value_counts()\n",
    "sns.countplot(x='Gender', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_range_order = [\"Age 18 to 24\", \"Age 25 to 29\", \"Age 30 to 34\", \"Age 35 to 39\", \"Age 40 to 44\", \"Age 45 to 49\", \"Age 50 to 54\", \"Age 55 to 59\", \"Age 60 to 64\", \"Age 65 to 69\", \"Age 70 to 74\", \"Age 75 to 79\", \"Age 80 or older\"]\n",
    "df[\"Age Range\"].value_counts()\n",
    "g = sns.countplot(x='Age Range', hue=\"Gender\", order=age_range_order ,data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "g.set_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_health_order = [\"Poor\", \"Fair\", \"Good\", \"Very good\", \"Excellent\"]\n",
    "df[\"General Health\"].value_counts()\n",
    "g = sns.countplot(x='General Health', hue=\"Gender\", order=general_health_order ,data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_cigarettes_frequency_order = [\"Never\", \"Not Right Now\", \"Some days\", \"Everyday\"]\n",
    "df[\"E-cigarettes Frequency\"].value_counts()\n",
    "g = sns.countplot(x='E-cigarettes Frequency', hue=\"Gender\", order=E_cigarettes_frequency_order ,data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smokeless_tobacco_order = [\"Not at all\", \"Some days\", \"Every day\"]\n",
    "df[\"Smokeless Tobacco Products\"].value_counts()\n",
    "g = sns.countplot(x='Smokeless Tobacco Products', hue=\"Gender\", order=smokeless_tobacco_order ,data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the data (Category feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_categories = [age_range_order, general_health_order, E_cigarettes_frequency_order, smokeless_tobacco_order]\n",
    "ordinal_features = [\"Age Range\", \"General Health\", \"E-cigarettes Frequency\", \"Smokeless Tobacco Products\"]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(categories=ordinal_categories)\n",
    "df[ordinal_features] = ordinal_encoder.fit_transform(df[ordinal_features])\n",
    "df[ordinal_features] = df[ordinal_features].astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = [\"Gender\", \"High Blood Pressure\", \"Kidney Disease\", \"Taking medicine for high cholesterol\", \"Angina or Coronary Heart Disease\", \n",
    "                   \"Heart Attack\", \"Stroke\", \"Diabetes\", \"Arthritis\", \"Smoked at Least 100 Cigarettes\", \"Cholesterol Is High\", \"Exercise in Past 30 Days\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for features in binary_features:\n",
    "    df[features] = label_encoder.fit_transform(df[features])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_correlations(corr_matrix):\n",
    "    summary = {\n",
    "        \"Very Strong\": [],\n",
    "        \"Strong\": [],\n",
    "        \"Moderate\": [],\n",
    "        \"Weak\": [],\n",
    "        \"Very Weak/No Correlation\": []\n",
    "    }\n",
    "\n",
    "    for col in corr_matrix.columns:\n",
    "        for idx in corr_matrix.index:\n",
    "            if col != idx:  # Avoid self-correlation\n",
    "                value = corr_matrix.loc[idx, col]\n",
    "                if abs(value) >= 0.9:\n",
    "                    summary[\"Very Strong\"].append((idx, col, value))\n",
    "                elif abs(value) >= 0.7:\n",
    "                    summary[\"Strong\"].append((idx, col, value))\n",
    "                elif abs(value) >= 0.5:\n",
    "                    summary[\"Moderate\"].append((idx, col, value))\n",
    "                elif abs(value) >= 0.3:\n",
    "                    summary[\"Weak\"].append((idx, col, value))\n",
    "                else:\n",
    "                    summary[\"Very Weak/No Correlation\"].append((idx, col, value))\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "correlation_summary = summarize_correlations(correlation_matrix)\n",
    "\n",
    "for category, correlations in correlation_summary.items():\n",
    "    print(f\"\\n{category} Correlations:\")\n",
    "    for feature1, feature2, value in correlations:\n",
    "        print(f\"{feature1} - {feature2}: {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age_BMI\"] = df[\"Age\"] * df[\"BMI\"]  # Interaction between age and BMI\n",
    "df[\"Exercise_BMI\"] = df[\"Exercise in Past 30 Days\"] * df[\"BMI\"]  # Effect of exercise on BMI\n",
    "df[\"Alcohol_Smoking\"] = df[\"Days in past 30 had alcoholic beverage\"] * df[\"Smoked at Least 100 Cigarettes\"]  # Relationship between drinking and smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Health_Risk_Score\"] = df[[\"High Blood Pressure\", \"Diabetes\", \"Taking medicine for high cholesterol\", \"Stroke\"]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "correlation_summary = summarize_correlations(correlation_matrix)\n",
    "\n",
    "for category, correlations in correlation_summary.items():\n",
    "    print(f\"\\n{category} Correlations:\")\n",
    "    for feature1, feature2, value in correlations:\n",
    "        print(f\"{feature1} - {feature2}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "target = \"Heart Attack\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "diabetes_X_train = scaler.fit_transform(X_train)\n",
    "diabetes_X_test = scaler.transform(X_test)\n",
    "\n",
    "####################################################################\n",
    "# Quick Helper Method - Find Best Parameters\n",
    "# Define parameter grid\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
    " \n",
    "# Perform grid search with cross-validation\n",
    "lasso_cv = GridSearchCV(linear_model.Lasso(), param_grid, cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    " \n",
    "# Print best parameter values and score\n",
    "print(\"Best Parameters:\", lasso_cv.best_params_)\n",
    "print(\"Best Score:\", lasso_cv.best_score_)\n",
    "####################################################################\n",
    "\n",
    "# Fit Lasso regression model\n",
    "lasso = linear_model.Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    " \n",
    "# Evaluate model performance on test set\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lasso.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Run K-Means for different K values\n",
    "inertia = []\n",
    "K_values = range(2, 11)  # Testing from 2 to 10 clusters\n",
    "\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(df)  # df_processed = Preprocessed dataset\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.plot(K_values, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "knee_locator = KneeLocator(range(2, 11), inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.knee\n",
    "\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df[\"Cluster\"] = kmeans.fit_predict(df)\n",
    "\n",
    "# Check cluster distribution\n",
    "df[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters based on important features\n",
    "sns.boxplot(x=\"Cluster\", y=\"BMI\", data=df)\n",
    "plt.title(\"BMI Distribution Across Clusters\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x=\"Cluster\", y=\"Age\", data=df)\n",
    "plt.title(\"Age Distribution Across Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(df)\n",
    "df[\"PCA1\"] = reduced_features[:, 0]\n",
    "df[\"PCA2\"] = reduced_features[:, 1]\n",
    "\n",
    "# Plot clusters with colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df[\"PCA1\"], y=df[\"PCA2\"], hue=df[\"Cluster\"], palette=\"tab10\", alpha=0.7)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"K-Means Clustering Visualization (PCA Reduced)\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Heart Attack\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Apply SVD for dimensionality reduction\n",
    "num_components = 10  # Adjust based on explained variance\n",
    "svd = TruncatedSVD(n_components=num_components)\n",
    "X_svd = svd.fit_transform(X_scaled)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_svd, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Choose the number of components (dimensions)\n",
    "# num_components = 2  # Reduce data to 2D for visualization\n",
    "\n",
    "# svd = TruncatedSVD(n_components=num_components)\n",
    "# X_svd = svd.fit_transform(df)\n",
    "\n",
    "# # Explained variance ratio\n",
    "# explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "\n",
    "# # Plot explained variance to decide on the number of components\n",
    "# plt.plot(range(1, num_components + 1), explained_variance, marker=\"o\")\n",
    "# plt.xlabel(\"Number of Components\")\n",
    "# plt.ylabel(\"Explained Variance\")\n",
    "# plt.title(\"Explained Variance vs. Number of SVD Components\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "svm_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_importance = np.abs(svd.components_).sum(axis=0)\n",
    "sorted_features = np.argsort(-feature_importance)\n",
    "\n",
    "print(\"Top SVD Components Contributing to Heart Attack Risk:\")\n",
    "print(sorted_features[:5])  # Show top 5 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine optimal K using the Elbow Method\n",
    "inertia = []\n",
    "K_values = range(2, 11)\n",
    "\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_svd)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.plot(K_values, inertia, marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal K (After SVD)\")\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal K and apply K-Means\n",
    "optimal_k = 4  # Example choice based on elbow point\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_svd)\n",
    "\n",
    "# Add cluster labels to the dataset\n",
    "df[\"Cluster_SVD\"] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_svd[:, 0], X_svd[:, 1], c=clusters, cmap=\"coolwarm\", edgecolors=\"k\", alpha=0.7)\n",
    "plt.xlabel(\"SVD Component 1\")\n",
    "plt.ylabel(\"SVD Component 2\")\n",
    "plt.title(\"Clusters After SVD Reduction\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
